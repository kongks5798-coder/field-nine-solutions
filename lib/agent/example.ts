// ---------------------------------------------------------
// Field Nine OS: Agent ì‹¤í–‰ ì˜ˆì‹œ (TypeScript)
// Next.js API Route ë˜ëŠ” Server Actionì—ì„œ ì‚¬ìš©
// ---------------------------------------------------------

import {
  AgentController,
  ToolRegistry,
  QualityAgent,
  OpenAIProvider,
  AgentLLMInterface,
  createDefaultTools,
} from './index';

// ============================================================
// 1. ê¸°ë³¸ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ
// ============================================================

export async function runBasicAgent() {
  // ë„êµ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìƒì„±
  const tools = createDefaultTools();

  // í’ˆì§ˆ ê²€ìˆ˜ ì—ì´ì „íŠ¸ (ì„ íƒì )
  const qualityAgent = new QualityAgent(undefined, 0.7, 2);

  // ì—ì´ì „íŠ¸ ìƒì„±
  const agent = new AgentController(
    {
      agentId: 'field-nine-agent',
      maxIterations: 5,
      qualityThreshold: 0.7,
      verbose: true,
      onStateChange: state => {
        console.log(`ğŸ“Œ State: ${state}`);
      },
      onIteration: (iteration, data) => {
        console.log(`\n--- Iteration ${iteration} ---`);
        console.log(`Thought: ${data.thought.reasoning.slice(0, 100)}...`);
      },
    },
    tools,
    undefined, // LLM ProviderëŠ” ë‚´ë¶€ì—ì„œ ìƒì„±
    qualityAgent
  );

  // íƒœìŠ¤í¬ ì‹¤í–‰
  const result = await agent.execute('ì˜¤ëŠ˜ì˜ íŒ¨ì…˜ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì¤˜');

  console.log('Result:', result);
  return result;
}

// ============================================================
// 2. OpenAI Provider ì§ì ‘ ì‚¬ìš© ì˜ˆì‹œ
// ============================================================

export async function runDirectOpenAI() {
  const provider = new OpenAIProvider({
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o',
    temperature: 0.7,
    onStreamChunk: content => {
      process.stdout.write(content);
    },
    onError: error => {
      console.error('LLM Error:', error.errorType, error.message);
    },
  });

  // ì¼ë°˜ ì±„íŒ…
  const response = await provider.chat([
    { role: 'system', content: 'ë„ˆëŠ” ì¹œì ˆí•œ AI ë¹„ì„œì•¼.' },
    { role: 'user', content: 'ì•ˆë…•í•˜ì„¸ìš”! Field Nineì— ëŒ€í•´ ì†Œê°œí•´ì£¼ì„¸ìš”.' },
  ]);

  console.log('Response:', response.content);
  console.log('Usage:', response.usage);

  return response;
}

// ============================================================
// 3. ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì‹œ
// ============================================================

export async function runStreamingChat() {
  const provider = new OpenAIProvider({
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o',
  });

  console.log('ğŸŒŠ Streaming response:');

  for await (const chunk of provider.chatStream([
    { role: 'user', content: 'í•œêµ­ ìŠ¤íŠ¸ë¦¿ íŒ¨ì…˜ì˜ íŠ¹ì§•ì„ ì„¤ëª…í•´ì¤˜' },
  ])) {
    if (chunk.content) {
      process.stdout.write(chunk.content);
    }

    if (chunk.isComplete) {
      console.log('\nâœ… Stream complete');
      if (chunk.toolCalls) {
        console.log('Tool calls:', chunk.toolCalls);
      }
    }
  }
}

// ============================================================
// 4. Function Calling ì˜ˆì‹œ
// ============================================================

export async function runFunctionCalling() {
  const provider = new OpenAIProvider({
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o',
  });

  // ë„êµ¬ ìŠ¤í‚¤ë§ˆ ì •ì˜
  const tools = [
    {
      name: 'get_weather',
      description: 'Get current weather for a location',
      parameters: {
        type: 'object' as const,
        properties: {
          location: { type: 'string', description: 'City name' },
          unit: {
            type: 'string',
            enum: ['celsius', 'fahrenheit'],
            default: 'celsius',
          },
        },
        required: ['location'],
      },
    },
    {
      name: 'search_products',
      description: 'Search for fashion products',
      parameters: {
        type: 'object' as const,
        properties: {
          query: { type: 'string', description: 'Search query' },
          category: {
            type: 'string',
            enum: ['tops', 'bottoms', 'shoes', 'accessories'],
          },
        },
        required: ['query'],
      },
    },
  ];

  const response = await provider.chat(
    [
      {
        role: 'system',
        content: 'You can check weather and search products.',
      },
      {
        role: 'user',
        content: 'ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì£¼ê³  ì¸ê¸° ìŠ¤ë‹ˆì»¤ì¦ˆ ê²€ìƒ‰í•´ì¤˜',
      },
    ],
    tools
  );

  console.log('Response:', response.content);
  console.log('Tool Calls:', response.toolCalls);

  // ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
  if (response.toolCalls.length > 0) {
    const messages: Array<{
      role: 'system' | 'user' | 'assistant' | 'tool';
      content: string;
      toolCallId?: string;
    }> = [
      {
        role: 'assistant',
        content: response.content || '',
      },
    ];

    // Mock tool results
    for (const tc of response.toolCalls) {
      let result: Record<string, unknown>;

      if (tc.name === 'get_weather') {
        result = { temperature: 15, condition: 'ë§‘ìŒ' };
      } else if (tc.name === 'search_products') {
        result = {
          products: [
            { name: 'Nike Air Max', price: 159000 },
            { name: 'Adidas Samba', price: 139000 },
          ],
        };
      } else {
        result = { error: 'Unknown tool' };
      }

      messages.push({
        role: 'tool',
        content: JSON.stringify(result),
        toolCallId: tc.id,
      });
    }

    // ìµœì¢… ì‘ë‹µ
    const finalResponse = await provider.chat(messages as any);
    console.log('Final Response:', finalResponse.content);
  }

  return response;
}

// ============================================================
// 5. Next.js API Route ì˜ˆì‹œ
// ============================================================

/**
 * Next.js API Routeì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ
 *
 * // app/api/agent/route.ts
 * import { NextRequest, NextResponse } from 'next/server';
 * import { runAgentTask } from '@/lib/agent/example';
 *
 * export async function POST(req: NextRequest) {
 *   const { task } = await req.json();
 *   const result = await runAgentTask(task);
 *   return NextResponse.json(result);
 * }
 */

export async function runAgentTask(task: string) {
  const llmInterface = new AgentLLMInterface({
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o',
    enableStreaming: false, // API Routeì—ì„œëŠ” ë¹„í™œì„±í™”
    verbose: false,
    onError: error => {
      console.error('Error:', error);
    },
  });

  const response = await llmInterface.think(task);

  return {
    success: true,
    content: response.content,
    toolCalls: response.toolCalls,
    usage: llmInterface.getStats(),
  };
}

// ============================================================
// 6. Server Action ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì‹œ
// ============================================================

/**
 * Next.js Server Actionì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš© ì˜ˆì‹œ
 *
 * // app/actions/agent.ts
 * 'use server';
 *
 * import { streamAgentThought } from '@/lib/agent/example';
 *
 * export async function* agentStream(task: string) {
 *   for await (const chunk of streamAgentThought(task)) {
 *     yield chunk;
 *   }
 * }
 */

export async function* streamAgentThought(task: string) {
  const provider = new OpenAIProvider({
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o',
  });

  const systemPrompt = `You are an autonomous AI agent.
Respond with JSON: { "reasoning": "...", "confidence": 0.0-1.0, "plan": [...] }`;

  for await (const chunk of provider.chatStream([
    { role: 'system', content: systemPrompt },
    { role: 'user', content: task },
  ])) {
    yield chunk;
  }
}

// ============================================================
// 7. ì—ëŸ¬ ë³µêµ¬ ì˜ˆì‹œ
// ============================================================

export async function runWithErrorRecovery() {
  const provider = new OpenAIProvider({
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o',
    maxRetries: 3,
    retryDelay: 1000,
    onError: error => {
      console.log(`âš ï¸ Error (retryable: ${error.retryable}):`, error.message);
    },
  });

  try {
    const response = await provider.chat([
      { role: 'user', content: 'í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€' },
    ]);

    console.log('Success:', response.content);
    console.log('Stats:', provider.getUsageStats());
  } catch (error) {
    console.error('Final error after retries:', error);
  }
}
