# ---------------------------------------------------------
# Field Nine OS: LLM Providers
# OpenAI, Anthropic ë“± LLM ì—°ë™ êµ¬í˜„
# ---------------------------------------------------------

from __future__ import annotations
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import (
    Any, AsyncGenerator, Callable, Dict, List,
    Optional, Union, Literal, TypedDict
)
from enum import Enum
import asyncio
import json
import time
import logging

# OpenAI SDK >= 1.0.0
from openai import AsyncOpenAI, OpenAI, APIError, RateLimitError, APITimeoutError
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionChunk,
    ChatCompletionMessage,
    ChatCompletionMessageToolCall,
)
from openai.types.chat.chat_completion_message_tool_call import Function

logger = logging.getLogger(__name__)


# ============================================================
# Type Definitions
# ============================================================

class MessageRole(str, Enum):
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"


@dataclass
class ChatMessage:
    """ì±„íŒ… ë©”ì‹œì§€"""
    role: MessageRole
    content: str
    name: Optional[str] = None
    tool_calls: Optional[List[Dict]] = None
    tool_call_id: Optional[str] = None


@dataclass
class ToolCall:
    """ë„êµ¬ í˜¸ì¶œ ìš”ì²­"""
    id: str
    name: str
    arguments: Dict[str, Any]


@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ"""
    content: Optional[str]
    tool_calls: List[ToolCall]
    finish_reason: str
    usage: Dict[str, int]
    model: str
    raw_response: Any = None


@dataclass
class StreamChunk:
    """ìŠ¤íŠ¸ë¦¬ë° ì²­í¬"""
    content: str
    is_complete: bool = False
    tool_calls: Optional[List[ToolCall]] = None
    finish_reason: Optional[str] = None


class LLMError(Exception):
    """LLM ê´€ë ¨ ì—ëŸ¬ ê¸°ë³¸ í´ë˜ìŠ¤"""
    def __init__(self, message: str, error_type: str, retryable: bool = False):
        super().__init__(message)
        self.error_type = error_type
        self.retryable = retryable


class TokenLimitError(LLMError):
    """í† í° í•œë„ ì´ˆê³¼ ì—ëŸ¬"""
    def __init__(self, message: str, used_tokens: int = 0, max_tokens: int = 0):
        super().__init__(message, "token_limit", retryable=True)
        self.used_tokens = used_tokens
        self.max_tokens = max_tokens


class RateLimitException(LLMError):
    """Rate Limit ì—ëŸ¬"""
    def __init__(self, message: str, retry_after: float = 60.0):
        super().__init__(message, "rate_limit", retryable=True)
        self.retry_after = retry_after


# ============================================================
# Abstract LLM Provider
# ============================================================

class LLMProvider(ABC):
    """LLM Provider ì¶”ìƒ í´ë˜ìŠ¤"""

    @abstractmethod
    async def chat(
        self,
        messages: List[ChatMessage],
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> LLMResponse:
        """ì±„íŒ… ì™„ì„± API í˜¸ì¶œ"""
        pass

    @abstractmethod
    async def chat_stream(
        self,
        messages: List[ChatMessage],
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncGenerator[StreamChunk, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… API í˜¸ì¶œ"""
        pass

    @abstractmethod
    def convert_tool_schema(self, tool_schema: Dict) -> Dict:
        """ë„êµ¬ ìŠ¤í‚¤ë§ˆë¥¼ í•´ë‹¹ LLM í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        pass


# ============================================================
# OpenAI Provider
# ============================================================

class OpenAIProvider(LLMProvider):
    """
    OpenAI API Provider (GPT-4o)

    Features:
    - Function Calling ì§€ì›
    - ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
    - ìë™ ì¬ì‹œë„ (Rate Limit, Timeout)
    - í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
    - ì—ëŸ¬ í•¸ë“¤ë§ & QualityAgent ì—°ë™

    Usage:
        provider = OpenAIProvider(
            api_key=os.getenv("OPENAI_API_KEY"),
            model="gpt-4o",
            max_retries=3
        )

        response = await provider.chat([
            ChatMessage(role=MessageRole.USER, content="Hello")
        ])
    """

    # ëª¨ë¸ë³„ í† í° í•œë„
    MODEL_TOKEN_LIMITS = {
        "gpt-4o": 128000,
        "gpt-4o-mini": 128000,
        "gpt-4-turbo": 128000,
        "gpt-4": 8192,
        "gpt-3.5-turbo": 16385,
    }

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-4o",
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        timeout: float = 60.0,
        on_token_usage: Optional[Callable[[Dict], None]] = None,
        on_error: Optional[Callable[[LLMError], None]] = None,
        on_stream_chunk: Optional[Callable[[str], None]] = None,
    ):
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.timeout = timeout

        # Callbacks
        self.on_token_usage = on_token_usage
        self.on_error = on_error
        self.on_stream_chunk = on_stream_chunk

        # Token tracking
        self.total_tokens_used = 0
        self.session_cost = 0.0

        # Initialize client
        self.client = AsyncOpenAI(
            api_key=api_key,
            timeout=timeout,
            max_retries=0  # ì§ì ‘ ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
        )

        # Sync client for non-async contexts
        self._sync_client = OpenAI(
            api_key=api_key,
            timeout=timeout,
            max_retries=0
        )

    async def chat(
        self,
        messages: List[ChatMessage],
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> LLMResponse:
        """
        ì±„íŒ… ì™„ì„± API í˜¸ì¶œ

        Args:
            messages: ì±„íŒ… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            tools: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ìŠ¤í‚¤ë§ˆ ë¦¬ìŠ¤íŠ¸
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (temperature, max_tokens ë“±)

        Returns:
            LLMResponse: ì‘ë‹µ ê°ì²´

        Raises:
            TokenLimitError: í† í° í•œë„ ì´ˆê³¼
            RateLimitException: Rate Limit ì´ˆê³¼
            LLMError: ê¸°íƒ€ API ì—ëŸ¬
        """
        formatted_messages = self._format_messages(messages)
        formatted_tools = [self.convert_tool_schema(t) for t in tools] if tools else None

        request_params = {
            "model": kwargs.get("model", self.model),
            "messages": formatted_messages,
            "temperature": kwargs.get("temperature", self.temperature),
        }

        if self.max_tokens or kwargs.get("max_tokens"):
            request_params["max_tokens"] = kwargs.get("max_tokens", self.max_tokens)

        if formatted_tools:
            request_params["tools"] = formatted_tools
            request_params["tool_choice"] = kwargs.get("tool_choice", "auto")

        # ì¬ì‹œë„ ë¡œì§
        last_error = None
        for attempt in range(self.max_retries + 1):
            try:
                response = await self.client.chat.completions.create(**request_params)
                return self._parse_response(response)

            except RateLimitError as e:
                last_error = RateLimitException(
                    str(e),
                    retry_after=self._extract_retry_after(e)
                )
                if self.on_error:
                    self.on_error(last_error)

                if attempt < self.max_retries:
                    wait_time = last_error.retry_after * (attempt + 1)
                    logger.warning(f"Rate limit hit. Waiting {wait_time}s before retry...")
                    await asyncio.sleep(wait_time)
                    continue

            except APITimeoutError as e:
                last_error = LLMError(str(e), "timeout", retryable=True)
                if self.on_error:
                    self.on_error(last_error)

                if attempt < self.max_retries:
                    await asyncio.sleep(self.retry_delay * (attempt + 1))
                    continue

            except APIError as e:
                # í† í° í•œë„ ì´ˆê³¼ ì²´í¬
                if "maximum context length" in str(e).lower():
                    last_error = TokenLimitError(
                        str(e),
                        used_tokens=self._estimate_tokens(formatted_messages),
                        max_tokens=self.MODEL_TOKEN_LIMITS.get(self.model, 0)
                    )
                else:
                    last_error = LLMError(str(e), "api_error", retryable=False)

                if self.on_error:
                    self.on_error(last_error)

                if not last_error.retryable:
                    raise last_error

                if attempt < self.max_retries:
                    await asyncio.sleep(self.retry_delay * (attempt + 1))
                    continue

            except Exception as e:
                last_error = LLMError(str(e), "unknown", retryable=False)
                if self.on_error:
                    self.on_error(last_error)
                raise last_error

        raise last_error

    async def chat_stream(
        self,
        messages: List[ChatMessage],
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… API í˜¸ì¶œ

        ë³´ìŠ¤ì—ê²Œ ì‹¤ì‹œê°„ìœ¼ë¡œ Thought ê³¼ì •ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë°

        Usage:
            async for chunk in provider.chat_stream(messages):
                print(chunk.content, end="", flush=True)
                if chunk.is_complete:
                    print()  # ì¤„ë°”ê¿ˆ
        """
        formatted_messages = self._format_messages(messages)
        formatted_tools = [self.convert_tool_schema(t) for t in tools] if tools else None

        request_params = {
            "model": kwargs.get("model", self.model),
            "messages": formatted_messages,
            "temperature": kwargs.get("temperature", self.temperature),
            "stream": True,
        }

        if self.max_tokens or kwargs.get("max_tokens"):
            request_params["max_tokens"] = kwargs.get("max_tokens", self.max_tokens)

        if formatted_tools:
            request_params["tools"] = formatted_tools
            request_params["tool_choice"] = kwargs.get("tool_choice", "auto")

        try:
            stream = await self.client.chat.completions.create(**request_params)

            collected_content = ""
            collected_tool_calls: Dict[int, Dict] = {}

            async for chunk in stream:
                delta = chunk.choices[0].delta if chunk.choices else None
                finish_reason = chunk.choices[0].finish_reason if chunk.choices else None

                if delta and delta.content:
                    collected_content += delta.content

                    # ì½œë°± í˜¸ì¶œ
                    if self.on_stream_chunk:
                        self.on_stream_chunk(delta.content)

                    yield StreamChunk(
                        content=delta.content,
                        is_complete=False
                    )

                # Tool calls ìˆ˜ì§‘
                if delta and delta.tool_calls:
                    for tc in delta.tool_calls:
                        idx = tc.index
                        if idx not in collected_tool_calls:
                            collected_tool_calls[idx] = {
                                "id": tc.id or "",
                                "name": tc.function.name if tc.function else "",
                                "arguments": ""
                            }
                        if tc.function and tc.function.arguments:
                            collected_tool_calls[idx]["arguments"] += tc.function.arguments

                # ì™„ë£Œ ì²´í¬
                if finish_reason:
                    tool_calls = []
                    for tc_data in collected_tool_calls.values():
                        try:
                            args = json.loads(tc_data["arguments"]) if tc_data["arguments"] else {}
                        except json.JSONDecodeError:
                            args = {}

                        tool_calls.append(ToolCall(
                            id=tc_data["id"],
                            name=tc_data["name"],
                            arguments=args
                        ))

                    yield StreamChunk(
                        content="",
                        is_complete=True,
                        tool_calls=tool_calls if tool_calls else None,
                        finish_reason=finish_reason
                    )

        except RateLimitError as e:
            error = RateLimitException(str(e), retry_after=self._extract_retry_after(e))
            if self.on_error:
                self.on_error(error)
            raise error

        except APIError as e:
            error = LLMError(str(e), "api_error", retryable=False)
            if self.on_error:
                self.on_error(error)
            raise error

    def convert_tool_schema(self, tool_schema: Dict) -> Dict:
        """
        ë„êµ¬ ìŠ¤í‚¤ë§ˆë¥¼ OpenAI Function Calling í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Input format (from tool_interface.py):
            {
                "name": "web_search",
                "description": "Search the web",
                "parameters": {
                    "type": "object",
                    "properties": {...},
                    "required": [...]
                }
            }

        Output format (OpenAI tools):
            {
                "type": "function",
                "function": {
                    "name": "web_search",
                    "description": "Search the web",
                    "parameters": {...}
                }
            }
        """
        return {
            "type": "function",
            "function": {
                "name": tool_schema.get("name", ""),
                "description": tool_schema.get("description", ""),
                "parameters": tool_schema.get("parameters", {
                    "type": "object",
                    "properties": {},
                    "required": []
                })
            }
        }

    def _format_messages(self, messages: List[ChatMessage]) -> List[Dict]:
        """ë©”ì‹œì§€ë¥¼ OpenAI API í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        formatted = []

        for msg in messages:
            message_dict = {
                "role": msg.role.value if isinstance(msg.role, MessageRole) else msg.role,
                "content": msg.content
            }

            if msg.name:
                message_dict["name"] = msg.name

            if msg.tool_calls:
                message_dict["tool_calls"] = msg.tool_calls

            if msg.tool_call_id:
                message_dict["tool_call_id"] = msg.tool_call_id

            formatted.append(message_dict)

        return formatted

    def _parse_response(self, response: ChatCompletion) -> LLMResponse:
        """API ì‘ë‹µ íŒŒì‹±"""
        message = response.choices[0].message
        finish_reason = response.choices[0].finish_reason

        # Tool calls íŒŒì‹±
        tool_calls = []
        if message.tool_calls:
            for tc in message.tool_calls:
                try:
                    args = json.loads(tc.function.arguments) if tc.function.arguments else {}
                except json.JSONDecodeError:
                    args = {}

                tool_calls.append(ToolCall(
                    id=tc.id,
                    name=tc.function.name,
                    arguments=args
                ))

        # Usage ì¶”ì 
        usage = {
            "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
            "completion_tokens": response.usage.completion_tokens if response.usage else 0,
            "total_tokens": response.usage.total_tokens if response.usage else 0,
        }

        self.total_tokens_used += usage["total_tokens"]
        self._update_cost(usage)

        if self.on_token_usage:
            self.on_token_usage(usage)

        return LLMResponse(
            content=message.content,
            tool_calls=tool_calls,
            finish_reason=finish_reason,
            usage=usage,
            model=response.model,
            raw_response=response
        )

    def _extract_retry_after(self, error: RateLimitError) -> float:
        """Rate Limit ì—ëŸ¬ì—ì„œ retry-after ì¶”ì¶œ"""
        # OpenAI ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ì¶”ì¶œ ì‹œë„
        error_str = str(error)
        if "Please retry after" in error_str:
            try:
                import re
                match = re.search(r"retry after (\d+)", error_str)
                if match:
                    return float(match.group(1))
            except:
                pass
        return 60.0  # ê¸°ë³¸ 60ì´ˆ

    def _estimate_tokens(self, messages: List[Dict]) -> int:
        """ë©”ì‹œì§€ í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )"""
        total_chars = sum(
            len(str(m.get("content", ""))) for m in messages
        )
        return total_chars // 4  # ëŒ€ëµ 4ìë‹¹ 1í† í°

    def _update_cost(self, usage: Dict):
        """ë¹„ìš© ê³„ì‚° (GPT-4o ê¸°ì¤€)"""
        # GPT-4o ê°€ê²© (2024ë…„ ê¸°ì¤€, ë³€ê²½ë  ìˆ˜ ìˆìŒ)
        input_cost_per_1k = 0.005  # $5 per 1M input tokens
        output_cost_per_1k = 0.015  # $15 per 1M output tokens

        input_cost = (usage.get("prompt_tokens", 0) / 1000) * input_cost_per_1k
        output_cost = (usage.get("completion_tokens", 0) / 1000) * output_cost_per_1k

        self.session_cost += input_cost + output_cost

    def get_usage_stats(self) -> Dict:
        """ì‚¬ìš©ëŸ‰ í†µê³„ ë°˜í™˜"""
        return {
            "total_tokens": self.total_tokens_used,
            "estimated_cost_usd": round(self.session_cost, 4),
            "model": self.model
        }

    def reset_usage_stats(self):
        """ì‚¬ìš©ëŸ‰ í†µê³„ ì´ˆê¸°í™”"""
        self.total_tokens_used = 0
        self.session_cost = 0.0


# ============================================================
# Streaming Thought Display
# ============================================================

class ThoughtStreamer:
    """
    ì—ì´ì „íŠ¸ Thought ê³¼ì • ì‹¤ì‹œê°„ í‘œì‹œ

    ë³´ìŠ¤ì—ê²Œ ì—ì´ì „íŠ¸ê°€ ë¬´ìŠ¨ ìƒê°ì„ í•˜ê³  ìˆëŠ”ì§€ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³´ì—¬ì¤Œ

    Usage:
        streamer = ThoughtStreamer(
            on_chunk=lambda c: print(c, end=""),
            on_complete=lambda full: save_to_log(full)
        )

        async for chunk in provider.chat_stream(messages):
            streamer.process(chunk)
    """

    def __init__(
        self,
        on_chunk: Optional[Callable[[str], None]] = None,
        on_complete: Optional[Callable[[str], None]] = None,
        on_tool_call: Optional[Callable[[ToolCall], None]] = None,
        prefix: str = "ğŸ’­ ",
        show_timestamp: bool = True
    ):
        self.on_chunk = on_chunk
        self.on_complete = on_complete
        self.on_tool_call = on_tool_call
        self.prefix = prefix
        self.show_timestamp = show_timestamp

        self.collected_content = ""
        self._started = False

    def process(self, chunk: StreamChunk) -> str:
        """ì²­í¬ ì²˜ë¦¬ ë° í‘œì‹œ"""
        if not self._started and chunk.content:
            self._started = True
            if self.show_timestamp:
                timestamp = time.strftime("%H:%M:%S")
                header = f"\n[{timestamp}] {self.prefix}"
            else:
                header = f"\n{self.prefix}"

            if self.on_chunk:
                self.on_chunk(header)

        if chunk.content:
            self.collected_content += chunk.content
            if self.on_chunk:
                self.on_chunk(chunk.content)

        if chunk.is_complete:
            self._started = False

            if self.on_complete:
                self.on_complete(self.collected_content)

            if chunk.tool_calls and self.on_tool_call:
                for tc in chunk.tool_calls:
                    self.on_tool_call(tc)

            result = self.collected_content
            self.collected_content = ""
            return result

        return ""

    def reset(self):
        """ìƒíƒœ ì´ˆê¸°í™”"""
        self.collected_content = ""
        self._started = False


# ============================================================
# Error Recovery Handler
# ============================================================

class ErrorRecoveryHandler:
    """
    ì—ëŸ¬ ë³µêµ¬ í•¸ë“¤ëŸ¬

    API ì—ëŸ¬ ë°œìƒ ì‹œ QualityAgentì™€ ì—°ë™í•˜ì—¬ ë³µêµ¬ ì „ëµ ê²°ì •

    Usage:
        handler = ErrorRecoveryHandler(quality_agent)
        recovery = await handler.handle_error(error, context)
        if recovery.should_retry:
            # ì¬ì‹œë„ ë¡œì§
    """

    def __init__(self, quality_agent: Optional['QualityAgent'] = None):
        self.quality_agent = quality_agent
        self.error_history: List[Dict] = []

    async def handle_error(
        self,
        error: LLMError,
        context: Dict
    ) -> 'RecoveryDecision':
        """ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ê²°ì •"""
        self.error_history.append({
            "error_type": error.error_type,
            "message": str(error),
            "timestamp": time.time(),
            "context": context
        })

        # ê¸°ë³¸ ë³µêµ¬ ì „ëµ
        decision = RecoveryDecision(
            should_retry=error.retryable,
            strategy="default",
            modifications={}
        )

        if isinstance(error, TokenLimitError):
            decision = self._handle_token_limit(error, context)

        elif isinstance(error, RateLimitException):
            decision = self._handle_rate_limit(error, context)

        # QualityAgentì—ê²Œ ë³´ê³ 
        if self.quality_agent:
            await self._report_to_quality_agent(error, decision)

        return decision

    def _handle_token_limit(
        self,
        error: TokenLimitError,
        context: Dict
    ) -> 'RecoveryDecision':
        """í† í° í•œë„ ì´ˆê³¼ ì²˜ë¦¬"""
        return RecoveryDecision(
            should_retry=True,
            strategy="reduce_context",
            modifications={
                "action": "truncate_history",
                "keep_last_n": 5,
                "summarize_old": True,
                "reason": f"Token limit exceeded: {error.used_tokens}/{error.max_tokens}"
            }
        )

    def _handle_rate_limit(
        self,
        error: RateLimitException,
        context: Dict
    ) -> 'RecoveryDecision':
        """Rate Limit ì²˜ë¦¬"""
        return RecoveryDecision(
            should_retry=True,
            strategy="wait_and_retry",
            modifications={
                "action": "delay",
                "wait_seconds": error.retry_after,
                "reason": "Rate limit exceeded"
            }
        )

    async def _report_to_quality_agent(
        self,
        error: LLMError,
        decision: 'RecoveryDecision'
    ):
        """QualityAgentì—ê²Œ ì—ëŸ¬ ë³´ê³ """
        if not self.quality_agent:
            return

        # QualityAgentì˜ ì»¤ìŠ¤í…€ ê·œì¹™ìœ¼ë¡œ ì—ëŸ¬ ì²˜ë¦¬ ê²°ê³¼ ê¸°ë¡
        report = {
            "error_type": error.error_type,
            "recovery_strategy": decision.strategy,
            "will_retry": decision.should_retry,
            "error_count": len(self.error_history)
        }

        logger.info(f"Error reported to QualityAgent: {report}")

    def get_error_stats(self) -> Dict:
        """ì—ëŸ¬ í†µê³„ ë°˜í™˜"""
        if not self.error_history:
            return {"total_errors": 0}

        error_types = {}
        for err in self.error_history:
            err_type = err["error_type"]
            error_types[err_type] = error_types.get(err_type, 0) + 1

        return {
            "total_errors": len(self.error_history),
            "by_type": error_types,
            "last_error": self.error_history[-1] if self.error_history else None
        }


@dataclass
class RecoveryDecision:
    """ë³µêµ¬ ê²°ì •"""
    should_retry: bool
    strategy: str
    modifications: Dict[str, Any]
    wait_seconds: float = 0.0


# ============================================================
# Integrated Agent LLM Interface
# ============================================================

class AgentLLMInterface:
    """
    ì—ì´ì „íŠ¸ìš© í†µí•© LLM ì¸í„°í˜ì´ìŠ¤

    OpenAIProvider + ErrorRecovery + Streamingì„ í†µí•©

    Usage:
        llm = AgentLLMInterface(
            api_key=os.getenv("OPENAI_API_KEY"),
            on_thought=lambda t: print(f"ğŸ’­ {t}")
        )

        # ì¼ë°˜ í˜¸ì¶œ
        response = await llm.think("íƒœìŠ¤í¬ ë¶„ì„")

        # ë„êµ¬ í˜¸ì¶œê³¼ í•¨ê»˜
        response = await llm.think_with_tools("ê²€ìƒ‰ í•„ìš”", tools)
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-4o",
        quality_agent: Optional['QualityAgent'] = None,
        on_thought: Optional[Callable[[str], None]] = None,
        on_tool_call: Optional[Callable[[ToolCall], None]] = None,
        on_error: Optional[Callable[[LLMError], None]] = None,
        enable_streaming: bool = True,
        verbose: bool = True
    ):
        self.enable_streaming = enable_streaming
        self.verbose = verbose
        self.on_thought = on_thought
        self.on_tool_call = on_tool_call

        # Provider ì´ˆê¸°í™”
        self.provider = OpenAIProvider(
            api_key=api_key,
            model=model,
            on_error=on_error,
            on_stream_chunk=self._handle_stream_chunk if enable_streaming else None
        )

        # Error Recovery
        self.error_handler = ErrorRecoveryHandler(quality_agent)

        # Thought Streamer
        self.thought_streamer = ThoughtStreamer(
            on_chunk=self._handle_thought_chunk,
            on_complete=self._handle_thought_complete,
            on_tool_call=on_tool_call
        )

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = """You are an autonomous AI agent operating within the Field Nine OS.
Your task is to reason step-by-step and execute actions using available tools.

When thinking, always structure your response as JSON:
{
    "reasoning": "Your step-by-step analysis",
    "confidence": 0.0-1.0,
    "plan": ["step1", "step2", ...],
    "next_action": "The tool to use next or 'complete' if done"
}

Always think in Korean when the task is in Korean."""

    async def think(
        self,
        task: str,
        context: Optional[List[ChatMessage]] = None,
        **kwargs
    ) -> LLMResponse:
        """
        ë‹¨ìˆœ ì¶”ë¡  (ë„êµ¬ ì—†ì´)

        Args:
            task: ìˆ˜í–‰í•  íƒœìŠ¤í¬
            context: ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
        """
        messages = [
            ChatMessage(role=MessageRole.SYSTEM, content=self.system_prompt),
        ]

        if context:
            messages.extend(context)

        messages.append(ChatMessage(role=MessageRole.USER, content=task))

        if self.enable_streaming:
            return await self._stream_response(messages)
        else:
            return await self.provider.chat(messages)

    async def think_with_tools(
        self,
        task: str,
        tools: List[Dict],
        context: Optional[List[ChatMessage]] = None,
        **kwargs
    ) -> LLMResponse:
        """
        ë„êµ¬ì™€ í•¨ê»˜ ì¶”ë¡ 

        Args:
            task: ìˆ˜í–‰í•  íƒœìŠ¤í¬
            tools: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ìŠ¤í‚¤ë§ˆ
            context: ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
        """
        messages = [
            ChatMessage(role=MessageRole.SYSTEM, content=self.system_prompt),
        ]

        if context:
            messages.extend(context)

        messages.append(ChatMessage(role=MessageRole.USER, content=task))

        try:
            if self.enable_streaming:
                return await self._stream_response(messages, tools)
            else:
                return await self.provider.chat(messages, tools)

        except LLMError as e:
            # ì—ëŸ¬ ë³µêµ¬ ì‹œë„
            recovery = await self.error_handler.handle_error(e, {"task": task})

            if recovery.should_retry:
                if recovery.strategy == "reduce_context":
                    # ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ í›„ ì¬ì‹œë„
                    reduced_context = context[-5:] if context else None
                    return await self.think_with_tools(task, tools, reduced_context)

                elif recovery.strategy == "wait_and_retry":
                    await asyncio.sleep(recovery.modifications.get("wait_seconds", 60))
                    return await self.think_with_tools(task, tools, context)

            raise

    async def _stream_response(
        self,
        messages: List[ChatMessage],
        tools: Optional[List[Dict]] = None
    ) -> LLMResponse:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
        collected_content = ""
        final_tool_calls = []
        final_finish_reason = ""

        async for chunk in self.provider.chat_stream(messages, tools):
            self.thought_streamer.process(chunk)

            if chunk.content:
                collected_content += chunk.content

            if chunk.is_complete:
                if chunk.tool_calls:
                    final_tool_calls = chunk.tool_calls
                final_finish_reason = chunk.finish_reason or "stop"

        return LLMResponse(
            content=collected_content,
            tool_calls=final_tool_calls,
            finish_reason=final_finish_reason,
            usage={},  # ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” usage ì •ë³´ ì—†ìŒ
            model=self.provider.model
        )

    def _handle_stream_chunk(self, content: str):
        """ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ í•¸ë“¤ëŸ¬"""
        if self.verbose:
            print(content, end="", flush=True)

    def _handle_thought_chunk(self, content: str):
        """Thought ì²­í¬ í•¸ë“¤ëŸ¬"""
        if self.on_thought:
            self.on_thought(content)

    def _handle_thought_complete(self, full_thought: str):
        """Thought ì™„ë£Œ í•¸ë“¤ëŸ¬"""
        if self.verbose:
            print()  # ì¤„ë°”ê¿ˆ

    def get_stats(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        return {
            "usage": self.provider.get_usage_stats(),
            "errors": self.error_handler.get_error_stats()
        }
